import dotenv from "dotenv";
import { HumanMessage } from "@langchain/core/messages";
import { interviewerGraph } from "./graph/interviewer";

// Load environment variables
dotenv.config();

// Set up LangSmith tracing (optional)
if (process.env.LANGCHAIN_TRACING_V2 === "true") {
  process.env.LANGCHAIN_TRACING_V2 = "true";
  process.env.LANGCHAIN_PROJECT = process.env.LANGCHAIN_PROJECT || "AI_Interview_Chatbot";
}

async function main() {
  console.log("ğŸš€ Starting AI Interview Chatbot Server...");
  
  try {
    // Test the graph with a simple message
    const initialState = {
      messages: [new HumanMessage("ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ì„ ì‹œì‘í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.")],
      user_context: {
        profile: {
          name: "í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì",
          experience_level: "ì¤‘ê¸‰",
          tech_stack: ["JavaScript", "React", "Node.js"],
          preferred_language: "Korean"
        },
        session_id: "test-session-001",
        timestamp: new Date().toISOString()
      }
    };

    console.log("ğŸ“ Testing interview graph...");
    
    // Stream the graph execution
    const stream = await interviewerGraph.stream(initialState, {
      streamMode: "values"
    });

    for await (const chunk of stream) {
      const lastMessage = chunk.messages[chunk.messages.length - 1];
      if (lastMessage) {
        console.log(`\nğŸ¤– ${lastMessage.content}`);
        console.log(`ğŸ“Š Current stage: ${chunk.flow_control.interview_stage}`);
      }
    }

    console.log("\nâœ… Graph test completed successfully!");
    
  } catch (error) {
    console.error("âŒ Error testing graph:", error);
  }
}

// Run the main function
if (require.main === module) {
  main().catch(console.error);
}

export { interviewerGraph }; 