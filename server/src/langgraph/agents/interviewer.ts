import { InterviewStateType } from "../../types/state.js";
import { HumanMessage, AIMessage, BaseMessage } from "@langchain/core/messages";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { RunnableLambda } from "@langchain/core/runnables";
import { createSupervisorAgent } from "./supervisorAgent.js";
import { InitialStateBuilder } from '../state/InitialStateBuilder.js';
import { InterviewDataService } from '../../services/InterviewDataService.js';


const model = new ChatGoogleGenerativeAI({
  model: "gemini-2.0-flash-exp",
  maxOutputTokens: 2048,
  temperature: 0.7,
  apiKey: process.env.GOOGLE_API_KEY,
});

/**
 * ìŠˆí¼ë°”ì´ì € ì—­í• ì„ í•˜ëŠ” Runnableì„ ìƒì„±í•©ë‹ˆë‹¤.
 * LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤.
 * @param model ì‚¬ìš©í•  ChatGoogleGenerativeAI ëª¨ë¸
 * @returns ìŠˆí¼ë°”ì´ì € Runnable
 */
const createSupervisorRunnable = (model: ChatGoogleGenerativeAI) => {
  const supervisorChain = createSupervisorAgent(model);

  const route = async (state: InterviewStateType): Promise<{ next: string }> => {
    const { messages } = state;
    const lastMessage = messages[messages.length - 1] as HumanMessage;

    console.log("ğŸ¤– Supervisor agent í˜¸ì¶œ ì¤‘...");
    console.log("ğŸ¤– Input:", lastMessage.content);

    const response = await supervisorChain.invoke({
      chat_history: messages.map((msg: BaseMessage) => `${msg._getType()}: ${msg.content}`).join('\n'),
      input: lastMessage.content,
      interview_stage: state.task.interview_stage
    });

    // Handle complex response content from Gemini
    const responseContent = (
      Array.isArray(response.content)
        ? response.content.map(part => (part as any).text || '').join('')
        : response.content
    ) as string;
    
    console.log("ğŸ¤– Supervisor ì›ë³¸ ì‘ë‹µ:", responseContent);
    
    const lowercasedContent = responseContent.toLowerCase().trim();
    console.log("ğŸ¤– ì •ê·œí™”ëœ ì‘ë‹µ:", lowercasedContent);
    
    let nextNode = "Interviewer"; // ê¸°ë³¸ê°’
    
    if (lowercasedContent.includes("finish")) {
      nextNode = "FINISH";
    } else if (lowercasedContent.includes("technical")) {
      nextNode = "technical_question_agent";
    } else if (lowercasedContent.includes("followup")) {
      nextNode = "followup_question_agent";
    } else if (lowercasedContent.includes("evaluate")) {
      nextNode = "evaluate_answer";
    } else {
      nextNode = "Interviewer";
    }
    
    console.log("ğŸ¤– ìµœì¢… ê²°ì •:", nextNode);
    
    return { next: nextNode };
  };

  return new RunnableLambda({ func: route });
};


const supervisorAgent = createSupervisorRunnable(model);

// Supervisor ë…¸ë“œëŠ” ë‹¤ìŒì— ì–´ë–¤ ì›Œì»¤ë¥¼ ì‹¤í–‰í• ì§€ ê²°ì •í•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
export const supervisorNode = async (state: InterviewStateType): Promise<Pick<InterviewStateType, 'next'>> => {
  console.log("ğŸ¯ Supervisor node ì‹¤í–‰ ì¤‘...");
  console.log("ğŸ¯ í˜„ì¬ ìƒíƒœ:", {
    interview_stage: state.task.interview_stage,
    messages_count: state.messages.length,
    last_message: state.messages[state.messages.length - 1]?.content
  });
  
  const supervisorResult = await supervisorAgent.invoke(state);
  console.log("ğŸ¯ Supervisor ê²°ì •:", supervisorResult);
  
  return {
    next: supervisorResult.next,
  };
};

// Interviewer ë…¸ë“œëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” íë¦„ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
export const interviewerNode = async (state: InterviewStateType) => {
  // ì´ ë…¸ë“œëŠ” í˜„ì¬ AIì˜ ì‘ë‹µì„ ìƒíƒœì— ì¶”ê°€í•˜ëŠ” ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  // ì§€ê¸ˆì€ ë‹¤ìŒ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
  return { messages: [new AIMessage("ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.")] };
};


// ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
export async function processUserInput(
  graph: any, // ReturnType<InterviewGraph['compile']>,
  state: InterviewStateType,
  userInput: string
): Promise<InterviewStateType> {
  console.log("ğŸ”„ Processing user input:", userInput);
  
  // ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì¶”ê°€í•©ë‹ˆë‹¤.
  const updatedState = {
    ...state,
    messages: [...state.messages, new HumanMessage(userInput)],
    task: {
      ...state.task,
      current_answer: userInput
    },
    evaluation: {
      ...state.evaluation,
      turn_count: state.evaluation.turn_count + 1
    }
  };

  // ì—…ë°ì´íŠ¸ëœ ìƒíƒœë¡œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
  const result = await graph.invoke(updatedState);
  
  console.log("âœ… Graph execution completed");
  return result;
}

// ì¸í„°ë·°ë¥¼ ì‹œì‘í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
export async function startInterview(
  graph: any, // ReturnType<InterviewGraph['compile']>,
  initialState?: Partial<InterviewStateType>
): Promise<InterviewStateType> {
  console.log("ğŸš€ Starting interview...");
  
  // ì˜ì¡´ì„±ì„ ìƒì„±í•˜ê³  ì£¼ì…í•©ë‹ˆë‹¤.
  const dataService = new InterviewDataService();
  const stateBuilder = new InitialStateBuilder(dataService);
  const state = await stateBuilder.build(initialState);

  // ì¸í„°ë·°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.
  const result = await graph.invoke(state);
  
  console.log("âœ… Interview started successfully");
  return result;
} 