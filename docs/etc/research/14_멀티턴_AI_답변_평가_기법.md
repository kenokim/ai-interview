# **멀티턴 대화형 AI 평가: 방법론적 및 실용적 통합**

## **서론: 정적 답변에서 동적 대화로의 패러다임 전환**

### **핵심 과제**

대규모 언어 모델(LLM)의 평가 패러다임은 정적(static)인 단일 응답의
품질을 측정하는 것에서 동적(dynamic)인 다회성 대화의 전체적인 흐름을
평가하는 방향으로 중대한 전환을 맞이하고 있습니다. 단일턴(single-turn)
평가는 하나의 프롬프트에 대한 LLM의 답변이 얼마나 정확하고 관련성이
있는지를 원자적으로 평가하는 데 중점을 둡니다.^1^ 이는 질문-답변(QA),
요약, 번역과 같은 작업에는 유용하지만, 사용자와 여러 차례 상호작용하며
맥락을 쌓아가는 챗봇과 같은 대화형 AI의 본질적인 성능을 측정하는 데에는
명백한 한계를 가집니다.

멀티턴(multi-turn) 대화 평가는 훨씬 더 복잡한 과제입니다. 이는 단순히
개별 답변의 합을 평가하는 것이 아니라, 대화 전체의 일관성(coherence),
맥락 유지(context maintenance), 기억력(memory), 그리고 정해진 역할을
고수하는 능력 등 상태(state)를 가지는 프로세스 자체를 평가해야
합니다.^3^ 대화가 진행됨에 따라 이전의 발화 내용은 새로운 답변을
생성하는 데 필수적인 맥락이 되며, 챗봇은 이 누적된 정보를 효과적으로
이해하고 활용할 수 있어야 합니다. 따라서 멀티턴 대화의 평가는 개별
답변의 품질을 넘어, 대화의 전체적인 궤적(trajectory)을 포괄적으로
분석하는 새로운 접근법을 요구합니다.^1^

### **\"대화 속에서 길을 잃는\" 현상**

견고한 멀티턴 평가 방법론이 단순한 개선 사항이 아니라 필수적인
요구사항인 이유는 \"대화 속에서 길을 잃는(Lost in Conversation)\" 현상
때문입니다. 최근 연구에 따르면, GPT-4와 같은 최첨단 LLM조차도 멀티턴
대화 환경에서는 단일턴 환경에 비해 성능이 현저하게 저하되며, 일부
연구에서는 최대 39%의 성능 하락을 보고했습니다.^2^

이러한 성능 저하의 핵심 원인은 모델의 근본적인 능력(aptitude) 저하보다는
신뢰성(unreliability)의 급격한 증가에 있습니다. LLM은 대화의 초기
단계에서 불완전한 정보를 바탕으로 섣부른 가정을 하고, 한번 잘못된 경로로
들어서면 이를 바로잡지 못한 채 이전의 잘못된 답변에 과도하게 의존하는
경향을 보입니다.^2^ 예를 들어, 초기에 사용자의 의도를 잘못 파악하면,
이후 사용자가 추가적인 정보를 제공하여 바로잡으려 해도 모델은 초기
가정을 폐기하지 못하고 대화 전체를 잘못된 방향으로 이끌어갑니다. 이는
멀티턴 대화의 품질을 평가하는 것이 왜 중요한지를 명확히 보여주는
핵심적인 문제입니다.

### **보고서의 목표 및 구조**

본 보고서는 LLM 기반 챗봇과 같은 멀티턴 대화형 AI를 평가하기 위한
포괄적이고 실용적인 가이드를 제공하는 것을 목표로 합니다. 이를 위해 학술
연구, 산업계의 모범 사례, 그리고 기술 문서를 종합하여 체계적인
프레임워크를 제시합니다.

보고서는 총 4부로 구성됩니다. **1부**에서는 멀티턴 평가의 분석적 토대를
마련하기 위해 핵심 평가 방법론과 다차원적 평가 지표 프레임워크를 상세히
다룹니다. **2부**에서는 이론을 실제에 적용할 수 있도록 돕는 주요
오픈소스 프레임워크와 표준화된 벤치마크를 비교 분석합니다. **3부**에서는
평가 과정에서 발생할 수 있는 함정과 한계, 특히 전통적인 지표의 취약성과
\'LLM-as-a-Judge\'의 편향성을 비판적으로 검토합니다. 마지막으로
**4부**에서는 평가 결과를 바탕으로 지속적인 개선을 이끌어내는 운영상의
모범 사례와 LLMOps 파이프라인 구축 전략을 제시합니다. 이 구조를 통해
독자들은 멀티턴 대화 평가의 이론적 기반부터 실용적인 적용까지 아우르는
깊이 있는 이해를 얻게 될 것입니다.

## **1부: 멀티턴 평가의 해부** {#부-멀티턴-평가의-해부}

이 장에서는 멀티턴 대화 평가 프로세스를 체계적으로 분해하여, \'어떻게\'
평가할 것인가(방법론)와 \'무엇을\' 평가할 것인가(지표)에 대한 포괄적인
프레임워크를 수립합니다. 이 두 가지 축을 통해 평가의 이론적 기반을
다지고, 실용적인 적용을 위한 초석을 마련합니다.

### **1.1 핵심 방법론적 접근법** {#핵심-방법론적-접근법}

멀티턴 대화 평가에는 크게 네 가지 핵심 방법론이 존재합니다. 이들은 상호
배타적인 관계가 아니라, 개발 생애주기의 각 단계에서 서로를 보완하며
사용될 수 있는 전략적 도구들입니다.

#### **1.1.1 대화 시뮬레이션 (자동화된 종단 간 테스트)** {#대화-시뮬레이션-자동화된-종단-간-테스트}

**개념:** 대화 시뮬레이션은 평가 대상 챗봇과 상호작용할 가상의
\'사용자\' 역할을 수행하는 두 번째 LLM을 활용하는 기법입니다.^1^ 이 가상
사용자는 사전에 정의된 페르소나와 목표에 따라 동적으로 대화 궤적을
생성하며, 이를 통해 개발자는 초기 질문부터 문제 해결까지의 전체 대화
과정을 자동화된 방식으로 확장성 있게 테스트할 수 있습니다.^1^

**구현:**

- **테스트 페르소나 정의:** 가상 사용자 LLM에게 구체적인 페르소나와
  > 목표를 부여하는 것이 핵심입니다. 예를 들어, \"진료 예약에 대해
  > 불안해하며 개인정보(전화번호) 공유를 꺼리는 환자\"와 같은 페르소나를
  > 설정하면, 평가 대상 챗봇이 미묘한 감정을 인식하고, 신뢰를 구축하며,
  > 적절한 수준의 끈기를 발휘하는 능력을 테스트할 수 있습니다.^9^

- **시뮬레이션 제어:** 시뮬레이션은 사전에 정의된 턴 수(max_turns)만큼
  > 실행되거나, 특정 stopping_condition(종료 조건)이 충족될 때까지
  > 계속될 수 있습니다. 이 종료 조건은 종종 \"사용자가 더 이상 도와줄
  > 것이 없다고 확인했는가?\"와 같이 대화의 목표 달성 여부를 판단하는 또
  > 다른 LLM 호출로 구현됩니다.^1^

- **프레임워크:** LangChain의 openevals 패키지는
  > create_llm_simulated_user와 같은 유틸리티를 제공하여 이러한
  > 시뮬레이션 과정을 용이하게 합니다.^1^ PromptLayer 역시 대화
  > 시뮬레이터 단계를 제공하여 체계적인 평가를 지원합니다.^9^

**장단점:** 이 방법의 가장 큰 장점은 초기 설정이 비교적 용이하고, 대화의
전체 과정을 종단 간(end-to-end)으로 커버할 수 있다는 점입니다.^1^ 하지만
시뮬레이션은 본질적으로 비결정적이어서, 동일한 테스트를 반복
실행하더라도 매번 다른 대화 내용과 평가 결과가 나올 수 있는 예측
불가능성과 일관성 부족이라는 단점을 가집니다.^10^

#### **1.1.2 LLM-as-a-Judge (확장 가능한 품질 평가)** {#llm-as-a-judge-확장-가능한-품질-평가}

**개념:** \'LLM-as-a-Judge\'는 GPT-4와 같이 강력한 \'심판(judge)\' LLM을
사용하여, 사전에 정의된 기준에 따라 생성된 대화의 품질을 평가(채점)하는
기법입니다.^9^ 이는 단일 지표가 아니라, 인간의 판단을 대규모로 근사하는
유연하고 확장 가능한 평가 기술로 이해해야 합니다.^15^

**구현:**

- **채점 방식:** 평가는 크게 두 가지 방식으로 이루어집니다. \*\*단일
  > 채점(single grading)\*\*은 하나의 대화를 독립적으로 평가하여 점수를
  > 매기는 방식이며, \*\*쌍대 비교(pairwise comparison)\*\*는 두 개의
  > 대화 중 어느 쪽이 더 나은지를 판단하는 방식입니다.^13^

- **평가 기준 세분화:** 이 기법을 가장 효과적으로 사용하는 방법은
  > \"대화가 좋았는가?\"와 같은 모호한 질문 대신, 복잡한 평가 기준을
  > 단순하고 명확한 단위로 나누는 것입니다. 예를 들어, \"사용자가
  > 전화번호를 제공했는가?\" 또는 \"에이전트가 적절하게 통화를
  > 종료했는가?\"와 같이 참/거짓(✅/❌)으로 판단 가능한 이진적
  > 휴리스틱이나, 1점에서 5점 사이의 낮은 정밀도의 척도를 사용하는 것이
  > 효과적입니다.^9^ Ragas 프레임워크의  
  > AspectCritic 지표는 자연어로 정의된 기준에 대해 이진 점수를
  > 반환함으로써 이러한 원칙을 잘 구현하고 있습니다.^16^

- **프롬프트 엔지니어링:** 심판 LLM의 평가 품질은 프롬프트의 설계에 크게
  > 좌우됩니다. 프롬프트에는 명확한 지침, 채점 루브릭, 그리고 필요에
  > 따라 소수의 예시(few-shot examples)가 포함되어야 합니다.^15^ 특히,
  > 심판 LLM에게 최종 점수를 내리기 전에 판단의 근거를 단계별로
  > 서술하도록 요청하는 \'사고의 연쇄(Chain-of-Thought, CoT)\'
  > 프롬프팅은 평가의 신뢰도를 크게 향상시킵니다.^15^

이 기법이 효과적인 이유는 과제의 분리에 있습니다. 처음부터 완벽한 응답을
생성하는 것은 인지적으로 어려운 작업이지만, 이미 생성된 응답이 특정
기준(예: 관련성, 정확성)을 충족하는지 평가하는 것은 상대적으로 더 쉬운
과제이기 때문입니다.^14^

#### **1.1.3 인간 참여 평가 (Ground Truth 확보)** {#인간-참여-평가-ground-truth-확보}

**개념:** 인간 평가자는 자동화된 시스템이 놓칠 수 있는 미묘한 뉘앙스,
특히 사용자 만족도나 대화 흐름의 자연스러움과 같은 질적 측면에 대한
\'최종 표준(gold standard)\'을 제공하는 필수적인 역할을 합니다.^18^
인간의 피드백은 자동화된 평가 지표의 타당성을 검증하고, LLM이 포착하지
못하는 대화의 질적 저하를 발견하는 데 결정적입니다.^22^

**구현:**

- **턴 레벨 vs. 대화 레벨 레이블링:** 평가는 개별 발화 단위로 세분화하여
  > 각 응답의 의도, 정확성 등을 평가하는 \'턴 레벨(turn-level)\' 방식과,
  > 대화 전체를 조망하며 총체적인 성공 여부를 판단하는 \'대화
  > 레벨(conversation-level)\' 방식으로 수행될 수 있습니다.^3^ Label
  > Studio와 같은 레이블링 플랫폼은 평가자가 개별 턴을 평가하는 동안
  > 전체 대화 내용을 참조할 수 있도록 인터페이스를 구성하여 맥락에
  > 기반한 정확한 평가를 지원합니다.^18^

- **루브릭 설계:** 체계적인 평가 루브릭(rubric)의 설계는 필수적입니다.
  > 루브릭에는 정확성(Accuracy), 관련성(Relevance), 일관성(Coherence),
  > 공감(Empathy), 정중함(Politeness), 과업 완수(Task Completion) 등
  > 명확하고 구체적인 평가 기준이 정의되어야 합니다.^20^ 더 나아가,
  > RUBICON 프레임워크는 레이블링된 대화 데이터로부터 도메인 특화된
  > 루브릭을 LLM을 사용하여 자동으로 생성하고, 이를 인간 평가자가
  > 활용하는 진보된 접근법을 제시합니다.^24^

**장단점:** 인간 평가는 가장 높은 품질의 평가 결과를 제공하지만, 비용과
시간이 많이 소요되고 확장성이 낮다는 명백한 단점이 있습니다. 또한,
평가자 간의 주관적 차이로 인해 일관성 있는 결과를 얻기 어려운 문제도
존재합니다.^23^

#### **1.1.4 라이브 A/B 테스트 (실세계 성능 측정)** {#라이브-ab-테스트-실세계-성능-측정}

**개념:** A/B 테스트는 개발된 챗봇의 두 가지 이상의 버전을 실제 사용자
그룹에 무작위로 노출시켜, 어떤 버전이 핵심 비즈니스 지표에 더 긍정적인
영향을 미치는지 통계적으로 검증하는 방법입니다.^27^ 이는 챗봇의 실질적인
효과를 측정하는 최종적인 시험대라고 할 수 있습니다.

**구현:**

- **절차:** 사용자를 무작위로 두 그룹으로 나누어, 통제 그룹(A)은 기존
  > 버전의 챗봇을, 실험 그룹(B)은 새로운 버전의 챗봇을 사용하게 합니다.
  > 이후 목표 달성률(Goal Completion Rate), 사용자 참여도(User
  > Engagement), 재사용률(Retention Rate), 전환율(Conversion Rates)과
  > 같은 핵심 성과 지표(KPI)를 측정하여 두 버전의 성능을 비교합니다.^29^

- **테스트 요소:** 테스트 대상은 대화 흐름, 환영 메시지의 문구, 응답
  > 스타일, UI 요소뿐만 아니라 기반이 되는 LLM 모델이나 프롬프트 자체도
  > 될 수 있습니다.^29^

- **도구:** VWO, Optimizely, AB Tasty와 같은 상용 플랫폼들은 이러한 A/B
  > 테스트를 설계하고 실행하는 데 필요한 인프라를 제공합니다.^27^

**장단점:** A/B 테스트는 실제 사용자의 행동에 기반한 데이터 중심의
명확한 증거를 제공한다는 강력한 장점이 있습니다.^32^ 그러나 통계적
유의성을 확보하기 위해 상당한 양의 트래픽이 필요하고, 결과를 얻기까지
시간이 오래 걸릴 수 있습니다. 또한, 시스템을 블랙박스로 취급하기 때문에
특정 버전이 왜 더 나은 성과를 냈는지에 대한 근본적인 원인을 설명해주지는
못합니다.^33^

#### **하이브리드 평가 퍼널**

네 가지 핵심 평가 방법론---시뮬레이션, LLM-as-a-Judge, 인간 평가, A/B
테스트---는 각각 확장성, 비용, 신뢰도 측면에서 명확한 장단점을
가집니다.^1^ 대화 시뮬레이션은 빠르고 저렴하지만 현실성이 떨어질 수 있고
^10^, 인간 평가는 가장 정확하지만 느리고 비용이 많이 듭니다.^23^
LLM-as-a-Judge는 인간의 판단을 확장 가능하게 대체하지만 편향성을
내포하고 있으며 ^35^, A/B 테스트는 실제 비즈니스 영향을 측정하지만
결과를 설명하는 데 한계가 있습니다.^32^

따라서 성숙한 개발팀은 이 중 하나만을 선택하는 것이 아니라, 개발
생애주기의 각 단계에 맞춰 이들을 순차적으로 결합하는 \'하이브리드 평가
퍼널(Hybrid Evaluation Funnel)\' 전략을 채택합니다.

1.  **1단계 (개발 단계):** 빠르고 비용 효율적인 대화 시뮬레이션과
    > LLM-as-a-Judge를 활용하여 수많은 잠재적 변경사항에 대해 신속하게
    > 반복 실험을 수행합니다. 이 단계의 목표는 심각한 성능
    > 저하(regression)를 조기에 발견하고 개선의 방향성을 설정하는
    > 것입니다.

2.  **2단계 (준비/스테이징 단계):** 1단계에서 가능성을 보인 소수의
    > 후보군에 대해 보다 엄격한 인간 평가를 수행합니다. 이를 통해
    > 자동화된 지표를 검증하고 최종 배포 버전을 결정하는 데 필요한 높은
    > 신뢰도의 \'Ground Truth\' 데이터를 확보합니다.

3.  **3단계 (운영 단계):** 최종 후보 버전을 실제 사용자에게 배포하여
    > 라이브 A/B 테스트를 진행하고, 비즈니스 KPI에 미치는 실질적인
    > 영향을 측정합니다.

결론적으로, 가장 효과적인 평가 전략은 단일 방법론에 의존하는 것이
아니라, 개발 단계별로 속도, 비용, 신뢰도의 균형을 맞추는 하이브리드,
다단계 프로세스를 구축하는 것입니다.

### **표 1: 멀티턴 평가 방법론 비교**

<table style="width:100%;">
<colgroup>
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th>방법론</th>
<th>설명</th>
<th>주요 장점</th>
<th>주요 단점</th>
<th>확장성/비용</th>
<th>이상적인 사용 사례</th>
</tr>
<tr class="odd">
<th><strong>대화 시뮬레이션</strong></th>
<th>LLM을 가상 사용자로 활용하여 동적으로 대화 궤적을 생성하고 종단 간
테스트를 수행 <sup>1</sup></th>
<th><p>- 빠른 초기 설정</p>
<p>- 종단 간 커버리지</p>
<p>- 반복 행동/맥락 손실 탐지 1</p></th>
<th><p>- 비결정성 및 일관성 부족</p>
<p>- 실제 사용자 행동과 다를 수 있음 10</p></th>
<th>높음 / 낮음</th>
<th>개발 초기 단계에서의 신속한 반복 테스트, 주요 회귀 테스트</th>
</tr>
<tr class="header">
<th><strong>LLM-as-a-Judge</strong></th>
<th>강력한 '심판' LLM을 사용하여 사전 정의된 기준에 따라 대화 품질을
자동 채점 <sup>9</sup></th>
<th><p>- 높은 확장성</p>
<p>- 비용 효율적</p>
<p>- 인간 판단과 높은 상관관계 14</p></th>
<th><p>- 위치, 장황함, 자기 선호 등 편향성 존재</p>
<p>- 프롬프트에 민감함 14</p></th>
<th>높음 / 중간</th>
<th>대규모 데이터셋에 대한 자동화된 품질 점수화, 인간 평가의 보조
수단</th>
</tr>
<tr class="odd">
<th><strong>인간 평가</strong></th>
<th>전문 평가자가 정해진 루브릭에 따라 대화 품질을 직접 평가하여 'Ground
Truth'를 생성 <sup>18</sup></th>
<th><p>- 가장 높은 정확도와 신뢰도</p>
<p>- 미묘한 뉘앙스(공감, 흐름) 포착 가능 21</p></th>
<th><p>- 높은 비용과 시간 소요</p>
<p>- 낮은 확장성</p>
<p>- 평가자 간 주관성 및 불일치 23</p></th>
<th>낮음 / 높음</th>
<th>자동화된 지표 검증, 최종 후보군에 대한 심층 분석, 'Ground Truth'
데이터셋 구축</th>
</tr>
<tr class="header">
<th><strong>라이브 A/B 테스트</strong></th>
<th>두 가지 이상의 챗봇 버전을 실제 사용자에게 무작위로 노출시켜
비즈니스 KPI를 비교 <sup>27</sup></th>
<th><p>- 실제 사용자 행동 기반의 명확한 증거</p>
<p>- 비즈니스 영향 직접 측정 32</p></th>
<th><p>- 통계적 유의성 확보에 많은 트래픽 필요</p>
<p>- 결과 도출까지 시간 소요</p>
<p>- '왜'에 대한 설명력 부족 33</p></th>
<th>중간 / 높음</th>
<th>운영 환경에서의 최종 성능 검증, 비즈니스 KPI에 미치는 영향 측정</th>
</tr>
</thead>
<tbody>
</tbody>
</table>

### **1.2 다차원적 대화형 AI 평가 지표 프레임워크** {#다차원적-대화형-ai-평가-지표-프레임워크}

효과적인 멀티턴 평가는 \'무엇을 측정할 것인가\'에 대한 명확한 정의에서
시작됩니다. 평가 지표는 크게 네 가지 범주로 나눌 수 있으며, 이들은 대화
품질의 다양한 측면을 포괄적으로 측정합니다.

#### **1.2.1 기초 품질 지표 (턴 레벨)** {#기초-품질-지표-턴-레벨}

이 지표들은 좋은 대화의 가장 기본적인 구성 요소로, 주로 각 턴(turn)
단위로 평가됩니다.

- 정확성 (Accuracy): 제공된 정보가 사실에 부합하는가? ^19^

- 관련성 (Relevance): 응답이 사용자의 직전 질문 및 이전 대화 맥락과
  > 관련이 있는가? ^20^

- 유창성 및 일관성 (Fluency & Coherence): 사용된 언어가 문법적으로
  > 자연스럽고, 이전 턴들과 논리적으로 일관되는가? ^19^

#### **1.2.2 맥락 및 기억력 지표 (멀티턴 특화)** {#맥락-및-기억력-지표-멀티턴-특화}

이 지표들은 멀티턴 대화의 핵심적인 어려움을 직접적으로 다룹니다.

- 역할 고수 (Role Adherence): 챗봇이 대화 내내 \"친절한 비서\"나
  > \"셰익스피어 풍의 시인\"과 같이 부여된 역할을 일관되게 유지하는가?
  > 전체 턴 중 역할을 준수한 턴의 비율로 계산됩니다.^3^

- 대화 관련성 (Conversation Relevancy): 단일턴 관련성보다 진보된
  > 개념으로, 이전 턴들의 슬라이딩 윈도우(sliding window)를 고려하여
  > 현재 응답이 진행 중인 대화 주제와 관련이 있는지 평가합니다.^3^

- 지식 유지 (Knowledge Retention): 챗봇이 이전 턴에서 사용자가 제공한
  > 정보를 기억하는가? 이 지표는 챗봇이 이미 받은 정보를 다시
  > 질문하는지를 확인하여 \'지식 마모(knowledge attrition)\'가 없는 턴의
  > 비율로 측정합니다.^3^

- 맥락 회상 / 맥락 유지 (Context Recall / Context Maintenance): 시스템이
  > 대화 기록 전체를 이해하고 활용하는 능력을 더 광범위하게
  > 측정합니다.^4^ MT-Bench-101과 같은 벤치마크는 이 능력을 명시적으로
  > 테스트합니다.^12^

#### **1.2.3 과업 지향 및 목표 달성 지표** {#과업-지향-및-목표-달성-지표}

이 지표들은 챗봇의 궁극적인 효과성과 유용성을 측정합니다.

- 과업 완수율 (Task Completion Rate): 사전에 정의된 과업이 성공적으로
  > 완료되었는지를 이진적으로 측정하는 지표로, 과업 지향
  > 대화(task-oriented dialogue) 연구에서 핵심적으로 사용됩니다.^4^

- 목표 달성률 (Goal Completion Rate, GCR): 예약, 구매 등 특정 비즈니스
  > 목표 행동의 성공률을 측정하는 핵심 성과 지표(KPI)입니다.^40^

- 최초 문의 해결률 (First Contact Resolution, FCR): 인간 상담원에게
  > 이관되지 않고 첫 번째 상호작용 내에서 문제가 해결된 비율을
  > 나타냅니다.^41^

- 셀프 서비스율 / 처리율 (Self-Service / Containment Rate): 인간의 개입
  > 없이 봇에 의해 완전히 처리된 상호작용의 비율로, 효율성을 나타내는
  > 주요 지표입니다.^40^

#### **1.2.4 사용자 경험 및 비즈니스 지표** {#사용자-경험-및-비즈니스-지표}

이 지표들은 챗봇의 성능을 실질적인 비즈니스 성과와 연결합니다.

- 사용자 만족도 (Customer Satisfaction, CSAT): 상호작용에 대한 사용자의
  > 만족도를 직접 측정하는 지표로, 주로 대화 종료 후 설문조사를 통해
  > 수집됩니다.^19^

- 순수 추천 지수 (Net Promoter Score, NPS): 사용자가 해당 서비스를
  > 타인에게 추천할 가능성을 측정하여 브랜드 충성도를 나타냅니다.^41^

- 상호작용률 / 대화 길이 (Interaction Rate / Conversation Length):
  > 교환된 평균 메시지 수로, 긍정적인 참여도(engagement)를 의미할 수도
  > 있지만, 사용자의 어려움을 나타낼 수도 있습니다.^40^

- 이탈률 (Bounce Rate): 단일 상호작용 후 세션이 종료되는 비율로,
  > 사용자의 불만이나 챗봇의 부적절한 배치를 시사합니다.^40^

- 상호작용당 비용 (Cost per Interaction): 봇 상호작용의 비용을 인간
  > 상담원의 비용과 비교하는 재무적 지표입니다.^41^

#### **내재적 지표와 외재적 지표 간의 긴장 관계**

위에 열거된 지표들은 두 가지 큰 범주로 나눌 수 있습니다. 첫 번째
그룹(기초 품질, 맥락/기억력)은 대화 자체의 \*\*내재적 품질(intrinsic
quality)\*\*을 측정합니다(예: 관련성, 일관성). 이들은 주로
LLM-as-a-Judge나 인간 평가자에 의해 평가됩니다. 두 번째 그룹(과업 지향,
비즈니스)은 대화의 **외재적 결과(extrinsic outcome)** 또는 영향을
측정합니다(예: GCR, CSAT). 이들은 주로 A/B 테스트나 분석 도구를 통해
측정됩니다.

여기에는 중요한 긴장 관계가 존재합니다. 어떤 챗봇은 내재적 지표상으로는
완벽한 대화(유창하고, 관련성 높고, 일관됨)를 수행하면서도 사용자의
목표를 달성해주지 못하거나 비즈니스 성과를 개선하지 못할 수 있습니다.
예를 들어, 매우 정중하고 일관되게 문제를 해결할 수 없다고 말하는 봇이
여기에 해당합니다. 반대로, 다소 삐걱거리는 대화가 결과적으로는 판매로
이어질 수도 있습니다.

이는 포괄적인 평가 프레임워크가 반드시 두 가지 측면을 모두 측정해야 함을
시사합니다. 많은 학술적 벤치마크가 집중하는 내재적 언어 품질에만
의존하는 것은 운영 환경의 시스템에는 불충분합니다. 반대로 일부 A/B
테스트처럼 외재적 비즈니스 지표에만 의존하는 것은 근본적인 대화 문제를
진단하고 수정하는 데 필요한 정보를 제공하지 못합니다.

따라서 성숙한 평가 파이프라인이 답해야 할 궁극적인 질문은 다음과
같습니다: **\"내재적 품질 지표(예: 지식 유지, 역할 고수)의 개선이 어떻게
외재적 비즈니스 지표(예: CSAT, FCR)의 향상으로 이어지는가?\"** 이 인과
관계를 규명하는 것이 평가의 최종 목표가 되어야 합니다.

### **표 2: 주요 멀티턴 평가 지표**

| 지표                                        | 범주           | 설명                                                                                                                      | 측정/계산 방법                                                                                                                                   | 지원 프레임워크                |
|---------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|
| **역할 고수 (Role Adherence)**              | 맥락/기억력    | 챗봇이 대화 내내 부여된 역할을 일관되게 유지하는지 평가 ^3^                                                               | LLM-as-a-Judge를 사용하여 각 턴이 역할을 준수했는지 평가 후, 전체 턴 대비 준수 턴의 비율 계산 ^3^                                                | DeepEval                       |
| **대화 관련성 (Conversation Relevancy)**    | 맥락/기억력    | 슬라이딩 윈도우 내의 이전 대화 맥락을 고려하여 현재 응답의 관련성을 평가 ^3^                                              | LLM-as-a-Judge가 슬라이딩 윈도우 내의 대화를 바탕으로 현재 턴의 관련성을 평가하고, 관련성 있는 턴의 비율 계산 ^3^                                | DeepEval                       |
| **지식 유지 (Knowledge Retention)**         | 맥락/기억력    | 챗봇이 사용자가 이전에 제공한 정보를 기억하는지, 즉 이미 받은 정보를 다시 묻지 않는지 평가 ^3^                            | LLM-as-a-Judge가 이전 대화에서 제공된 지식 목록을 추출하고, 현재 턴에서 해당 정보를 다시 묻는지 확인하여 \'지식 마모\'가 없는 턴의 비율 계산 ^3^ | DeepEval                       |
| **과업 완수율 (Task Completion Rate)**      | 과업 지향      | 사전에 정의된 과업(예: 정보 검색, 예약)이 성공적으로 완료되었는지 이진적으로 평가 ^4^                                     | 인간 평가자 또는 LLM-as-a-Judge가 대화 로그를 분석하여 과업 성공 여부를 \'성공/실패\'로 판단 ^4^                                                 | 학술 벤치마크, 커스텀 평가     |
| **목표 달성률 (Goal Completion Rate, GCR)** | 비즈니스       | 사용자가 챗봇을 통해 특정 비즈니스 목표(예: 구매, 양식 제출)를 달성한 비율 ^40^                                           | (목표 달성 세션 수 / 전체 세션 수) \* 100. 웹 분석 도구나 백엔드 로그를 통해 측정 ^40^                                                           | A/B 테스트 플랫폼, 분석 도구   |
| **사용자 만족도 (CSAT)**                    | 사용자 경험    | 사용자가 챗봇과의 상호작용에 얼마나 만족했는지를 직접 측정 ^19^                                                           | 대화 종료 후 \"만족하셨습니까?\"와 같은 질문에 대한 평가 점수(예: 1-5점)의 평균 ^41^                                                             | 인앱 설문조사, 피드백 메커니즘 |
| **셀프 서비스율 (Self-Service Rate)**       | 과업 지향      | 인간 상담원의 개입 없이 챗봇에 의해 완전히 해결된 문의의 비율 ^40^                                                        | (봇이 해결한 문의 수 / 전체 문의 수) \* 100. 헬프데스크 시스템 로그 분석 ^40^                                                                    | 분석 도구, 헬프데스크 시스템   |
| **AspectCritic**                            | 과업 지향/맥락 | 자연어로 정의된 특정 성공 기준(예: \"AI가 재요청 없이 모든 요청을 완료했는가?\")을 대화가 충족하는지 이진적으로 평가 ^16^ | LLM-as-a-Judge가 주어진 기준에 따라 대화 전체를 평가하고 1(충족) 또는 0(미충족)을 반환 ^16^                                                      | Ragas                          |

## **2부: 실무자 툴킷: 프레임워크와 벤치마크** {#부-실무자-툴킷-프레임워크와-벤치마크}

이 장에서는 이론을 넘어 실제 개발 현장에서 활용할 수 있는 구체적인
도구와 표준화된 테스트를 소개합니다. 멀티턴 대화 평가를 위한 주요
오픈소스 프레임워크를 비교 분석하고, 자체 개발한 챗봇의 성능을 업계 최고
수준과 비교할 수 있는 공개 벤치마크를 살펴봅니다.

### **2.1 오픈소스 평가 프레임워크 비교 분석** {#오픈소스-평가-프레임워크-비교-분석}

멀티턴 평가를 지원하는 다양한 오픈소스 프레임워크가 있으며, 각각은
고유한 철학과 강점을 가지고 있습니다. 이들은 단순히 동일한 작업을 다른
방식으로 수행하는 도구가 아니라, 평가 문제의 각기 다른 측면에 집중하는
전문화된 툴킷으로 이해할 수 있습니다.

#### **2.1.1 DeepEval: 유닛 테스트로서의 평가** {#deepeval-유닛-테스트로서의-평가}

**철학:** DeepEval은 LLM 평가를 소프트웨어 개발의 유닛 테스트(Unit
Test)와 같은 엄격한 공학적 원칙으로 접근합니다. \"LLM을 위한
Pytest\"라는 슬로건은 이러한 개발자 중심의 철학을 잘 보여줍니다.^42^

**멀티턴 기능:** 이 프레임워크는 ConversationalTestCase와 같은 클래스를
제공하여 멀티턴 대화 평가를 명시적으로 지원합니다. 특히
RoleAdherenceMetric, ConversationRelevancyMetric,
KnowledgeRetentionMetric과 같은 멀티턴 특화 지표들을 내장하고 있으며,
코드 예제와 함께 제공되어 개발자가 쉽게 자신의 테스트 워크플로우에
통합할 수 있습니다.^3^

**강점:** 개발자 워크플로우(특히 Pytest)와의 강력한 통합, 점수가 낮은
이유를 설명해주는 \'자기 설명적(self-explaining)\' 지표, 그리고 RAG 및
일반 목적 평가를 아우르는 광범위한 지표 포트폴리오가 주요
강점입니다.^43^

#### **2.1.2 Ragas: RAG 평가 전문 프레임워크** {#ragas-rag-평가-전문-프레임워크}

**철학:** Ragas는 이름에서 알 수 있듯이 검색 증강
생성(Retrieval-Augmented Generation, RAG) 파이프라인 평가에 특화된
프레임워크입니다.^42^ 많은 현대 챗봇이 RAG 아키텍처를 기반으로 한다는
점에서 그 중요성이 큽니다.

**멀티턴 기능:** RAG에 중점을 두지만, AspectCritic 지표는 멀티턴 평가에
매우 효과적입니다. 이 지표는 \"AI가 재요청 없이 모든 인간의 요청을
완전히 완료했는가?\"와 같이 성공 기준을 자연어로 정의하고 이진
점수(1/0)를 반환합니다. 이를 통해 대화 전체에 걸쳐 발생하는 \'망각\'이나
\'규정 준수 위반\'과 같은 문제를 체계적으로 검사할 수 있습니다.^16^
Ragas 문서는 은행 챗봇의 어조(tonality)나 과업 완수 여부를 평가하는
구체적인 예제를 제공합니다.^16^

**강점/약점:** RAG 기반 애플리케이션 평가에 매우 강력한 성능을 보입니다.
그러나 지표 자체가 점수에 대한 상세한 이유를 제공하지 않아, 낮은 점수를
받았을 때 디버깅이 DeepEval에 비해 어려울 수 있습니다.^43^

#### **2.1.3 LangChain/LangSmith & OpenEvals: 생태계 접근 방식** {#langchainlangsmith-openevals-생태계-접근-방식}

**철학:** LangSmith는 LLM 애플리케이션을 위한 포괄적인 관찰
가능성(observability) 및 평가 플랫폼이며, OpenEvals는 상호작용을
시뮬레이션하고 평가하기 위한 오픈소스 패키지입니다.^1^ 이들은
LangChain이라는 거대한 생태계 안에서 유기적으로 작동합니다.

**멀티턴 기능:** openevals 패키지는 멀티턴 상호작용을 시뮬레이션하고 그
결과로 생성된 전체 대화 궤적(trajectory)을 평가하는 데 특화되어
있습니다.^1^ LangSmith는 이러한 시뮬레이션을 \'실험(experiment)\'으로
관리하여 시간 경과에 따른 성능 변화를 추적할 수 있게 해줍니다. 특히 대화
로그 전체를 평가하는

trajectory_evaluators를 지원하여 멀티턴 평가를 용이하게 합니다.^1^

**강점:** 널리 사용되는 LangChain 생태계와의 긴밀한 통합이 가장 큰
장점입니다. LangSmith는 트레이싱, 디버깅, 데이터셋 및 평가 관리를 위한
강력한 UI를 제공하여 포괄적인 솔루션 역할을 합니다.^1^ 다만, 이는 관리형
서비스(managed service)이므로, 설정이 간편하다는 장점과 순수한
오픈소스가 아니라는 단점을 동시에 가집니다.^42^

#### **2.1.4 TruLens: 중간 단계 평가** {#trulens-중간-단계-평가}

**철학:** TruLens는 최종 결과물뿐만 아니라, LLM 애플리케이션의 실행 흐름
내 \'중간 단계(intermediate steps)\'---예를 들어, 에이전트의
계획(plan)이나 도구 호출(tool calls)---에 대한 질적 분석에 중점을
둡니다.^42^

**멀티턴 기능:** TruLens는 애플리케이션을 감싸(wrapping) 검색된
컨텍스트, 도구 호출, 에이전트 계획 등 모든 실행 단계를 기록하고
평가합니다.^45^ 이는 복잡한 멀티턴 에이전트를 디버깅하는 데 매우
중요합니다. 최종 응답의 실패가 언어 생성 단계가 아닌, 초기 단계의 잘못된
도구 호출이나 부적절한 컨텍스트 검색에서 비롯될 수 있기 때문입니다.^47^
TruLens는 이러한 중간 단계를 평가하기 위해 \'피드백 함수(feedback
functions)\'라는 개념을 사용합니다.^45^

**강점:** 복잡한 에이전트 워크플로우의 내부 로직을 투명하게 들여다보는
\'화이트박스(white-box)\' 평가에 독보적인 강점을 가집니다. 이는 복잡한
멀티턴 시스템 평가와 직결됩니다. 또한, OpenTelemetry와의 상호 운용성을
통해 기존 관찰 가능성 스택에 쉽게 통합할 수 있습니다.^45^

결론적으로, 개발자는 자신의 주요 요구사항에 따라 프레임워크를 선택할 수
있습니다. 코드 레벨의 엄격한 유닛 테스트를 원한다면 DeepEval을, RAG 중심
애플리케이션을 평가한다면 Ragas를, 완전한 추적 기능을 갖춘 플랫폼을
원한다면 LangSmith를, 복잡한 에이전트의 내부 로직을 디버깅하고 싶다면
TruLens를 선택하는 것이 합리적입니다. 이들은 상호 보완적으로 함께 사용될
수도 있습니다.

### **표 3: 오픈소스 평가 프레임워크 한눈에 보기**

| 프레임워크                | 핵심 철학                                                                 | 주요 멀티턴 기능                                                                                    | 이상적인 사용 사례                                                                                     | 통합 스타일                            |
|---------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|----------------------------------------|
| **DeepEval**              | 평가를 코드 내 유닛 테스트처럼 취급 (\"Pytest for LLMs\") ^42^            | ConversationalTestCase, RoleAdherenceMetric, KnowledgeRetentionMetric 등 내장 멀티턴 지표 ^3^       | 개발 워크플로우에 평가를 깊숙이 통합하고, 엄격한 코드 레벨의 회귀 테스트를 수행하고자 할 때            | Python 라이브러리 (Pytest 플러그인)    |
| **Ragas**                 | RAG(검색 증강 생성) 파이프라인 평가에 특화 ^42^                           | AspectCritic을 통해 자연어로 정의된 성공 기준(예: 망각, 어조)에 대한 대화 전체 평가 ^16^            | RAG 기반 챗봇의 검색 및 생성 품질을 심층적으로 평가하고, 특정 대화 속성을 검증하고자 할 때             | Python 라이브러리                      |
| **LangSmith & OpenEvals** | LLM 애플리케이션 생애주기 전반을 관리하는 관찰 가능성 및 평가 플랫폼 ^42^ | openevals를 통한 대화 시뮬레이션, LangSmith의 trajectory_evaluators를 통한 전체 대화 로그 평가 ^1^  | LangChain 생태계를 사용하며, 트레이싱부터 데이터셋 관리, 평가까지 포괄적인 플랫폼을 원할 때            | 관리형 서비스 (SaaS) + 오픈소스 패키지 |
| **TruLens**               | LLM 애플리케이션의 중간 단계(체인, 에이전트 계획, 도구 호출)를 평가 ^42^  | \'피드백 함수\'를 통해 검색된 컨텍스트, 도구 호출 등 에이전트의 내부 실행 흐름을 단계별로 평가 ^45^ | 복잡한 멀티턴 에이전트의 최종 결과뿐만 아니라, 내부 의사결정 과정의 오류를 진단하고 디버깅하고자 할 때 | Python 라이브러리 (OpenTelemetry 호환) |

### **2.2 표준화된 벤치마킹 및 경쟁 분석** {#표준화된-벤치마킹-및-경쟁-분석}

자체 개발한 챗봇의 성능을 객관적으로 파악하고 업계 최고 수준의 모델들과
비교하기 위해서는 표준화된 공개 벤치마크의 활용이 필수적입니다.

#### **2.2.1 MT-Bench: 구조화된 멀티턴 벤치마크** {#mt-bench-구조화된-멀티턴-벤치마크}

**방법론:** MT-Bench는 글쓰기, 역할극, 추론, 수학 등 8개 핵심 영역에
걸쳐 챗봇 어시스턴트를 평가하기 위해 설계된 80개의 도전적인 멀티턴 질문
세트입니다.^12^ 평가는 주로 GPT-4를 \'심판\'으로 사용하여 응답의 점수를
매기는 방식으로 이루어집니다.^48^

**진화 (MT-Bench-101):** 후속 버전인 MT-Bench-101은 훨씬 더 세분화된
평가를 제공합니다. 이 벤치마크는 맥락 기억력, 상호작용성(명확화를 위한
질문 능력), 적응성 등 13개의 구체적인 멀티턴 능력을 포함하는 계층적 분류
체계를 도입했습니다.^12^ 이를 통해 모델의 대화 능력에 대한 강점과 약점을
훨씬 더 상세하게 진단할 수 있습니다.

**중요성:** MT-Bench는 다양한 모델의 멀티턴 대화 능력을 표준화되고 재현
가능한 방식으로 측정하고 비교할 수 있는 기준을 제공한다는 점에서 큰
의미를 가집니다.^12^

#### **2.2.2 Chatbot Arena & Elo 평점 시스템** {#chatbot-arena-elo-평점-시스템}

**방법론:** Chatbot Arena는 사용자가 두 개의 익명 모델과 나란히
상호작용한 후, 더 나은 응답에 투표하는 크라우드소싱 기반
플랫폼입니다.^12^ 이러한 \'블라인드\' 비교 방식은 특정 모델 이름에 대한
사용자의 선입견을 완화시켜 객관적인 평가를 유도합니다.

**Elo 평점 시스템:** 본래 체스에서 유래한 Elo 시스템은 이러한 쌍대 비교
전투의 결과를 바탕으로 각 모델의 상대적인 능력 점수를 계산합니다.^54^
모델의 평점은 경기 결과와 상대방의 평점에 따라 변동되며, 더 강한 상대를
이겼을 때 더 많은 점수를 얻게 됩니다.^53^ 더 안정적인 순위를 위해
브래들리-테리(Bradley-Terry) 모델이 Elo 시스템과 함께 또는 대안으로
사용되기도 합니다.^49^

**중요성:** Chatbot Arena는 학술적인 테스트 성능이 아닌, 실제 사용자의
선호도를 반영하는 동적인 순위를 제공합니다.^54^ 이는 LLM 커뮤니티에서
가장 권위 있는 리더보드 중 하나로 인정받고 있으며, 모델의 실세계
매력도를 측정하는 중요한 척도가 됩니다.^54^

## **3부: 함정 탐색: 과제와 한계** {#부-함정-탐색-과제와-한계}

이 장에서는 멀티턴 대화 평가에 내재된 어려움과 한계를 비판적으로
검토합니다. 전통적인 지표의 취약성부터 최신 기법인 LLM-as-a-Judge의
편향성에 이르기까지, 개발자가 흔히 겪는 실수를 피하고 평가 결과의
신뢰도를 높이기 위한 \"눈을 크게 뜨고\" 보는 관점을 제공합니다.

### **3.1 대화 평가에서 전통적 지표의 취약성** {#대화-평가에서-전통적-지표의-취약성}

**\"일대다(One-to-Many)\" 문제:** 대화 평가의 근본적인 어려움은 주어진
하나의 대화 맥락에 대해 수많은 정답 응답이 존재할 수 있다는 점입니다.
훌륭한 응답임에도 불구하고, 사전에 정의된 참조(reference) 응답과 단어
수준에서 겹치는 부분이 전혀 없을 수 있습니다.^25^

**인간 판단과의 낮은 상관관계:** 기계 번역이나 요약 작업을 위해 개발된
BLEU, ROUGE, METEOR와 같은 n-gram 기반 지표들은 대화 품질에 대한 인간의
평가와 상관관계가 매우 낮다는 사실이 수많은 연구를 통해
입증되었습니다.^26^ 이러한 지표들은 표면적인 단어 일치 여부만 보기
때문에, 의미적으로는 타당하지만 표현이 다른 응답의 가치를 제대로
평가하지 못합니다.

**모델 기반 지표의 한계 (예: BERTScore):** BERTScore와 같이 의미적
유사성을 포착하는 모델 기반 지표는 n-gram 기반 지표보다 발전된
형태이지만, 여전히 참조 응답에 의존하기 때문에 \'일대다\' 문제를
근본적으로 해결하지 못합니다.^60^ 또한, 이들 지표는 사전 훈련된 모델이
가진 편향성을 그대로 물려받으며, 계산 비용이 높다는 단점이 있습니다.^60^
특히, 적대적 예제(adversarial examples)에 대해서는 성능이 현저히
저하되는 취약점을 보입니다.^62^

### **3.2 LLM-as-a-Judge의 편향성 해부** {#llm-as-a-judge의-편향성-해부}

LLM-as-a-Judge는 확장성 있는 평가 방법으로 각광받고 있지만, 그 판단이
왜곡될 수 있는 여러 체계적인 편향성을 내포하고 있습니다. 이러한 편향을
이해하고 완화하는 것은 신뢰할 수 있는 평가 결과를 얻기 위해
필수적입니다.

**체계적 편향성:** 연구를 통해 다음과 같은 일관된 편향이 발견되었습니다
^14^:

- 위치 편향 (Position Bias): 쌍대 비교 시, 내용과 무관하게 첫 번째로
  > 제시된 응답을 선호하는 경향입니다.^14^ 이를 완화하기 위해 응답의
  > 순서를 바꿔 두 번 평가하는 전략이 사용됩니다.^49^

- 장황함 편향 (Verbosity Bias): 더 간결하고 핵심적인 응답보다, 더 길고
  > 상세한 응답을 선호하는 경향입니다.^14^

- 자기 선호 편향 (Narcissistic Bias): 심판 모델이 자기 자신 또는 자신과
  > 같은 계열의 모델이 생성한 응답을 더 좋게 평가하는 경향입니다.^14^
  > 이를 완화하기 위해 생성 모델과 평가 모델을 다르게 사용하는 것이
  > 권장됩니다.^17^

- 과잉 정렬 (Over-alignment): 유용성과 무해성을 기준으로 훈련된 심판
  > LLM이, 사실적으로는 정확하지만 표현이 다소 직설적이거나 \'도움이 덜
  > 되는\' 것처럼 보이는 응답에 불이익을 주어 평가의 사각지대를 만들 수
  > 있습니다.^35^

**정확성 문제:** 심판 LLM은 스스로 정답을 알지 못하는 문제에 대해 다른
응답의 사실적 정확성을 신뢰성 있게 평가할 수 없습니다. 이 문제에 대한
가장 효과적인 해결책은 심판 LLM에게 고품질의, 인간이 작성한 참조 정답을
함께 제공하는 것입니다.^13^ 연구에 따르면, 성능이 낮은 심판 모델이라도
좋은 참조 정답을 받으면, 강력한 심판 모델이 참조 정답 없이 평가하는
것보다 인간의 판단과 더 높은 일치도를 보였습니다.^13^

**비일관성 및 프롬프트 민감성:** LLM 심판은 비결정적이어서 동일한 입력에
대해서도 시간에 따라 다른 점수를 부여할 수 있습니다.^14^ 또한, 평가
프롬프트의 미세한 문구 차이에도 판단 결과가 크게 달라지는 민감성을
보입니다.^17^

### **표 4: LLM-as-a-Judge의 주요 편향과 완화 전략**

<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>편향</th>
<th>영향 설명</th>
<th>증거 (출처 ID)</th>
<th>완화 전략</th>
<th></th>
</tr>
<tr class="odd">
<th><strong>위치 편향 (Position Bias)</strong></th>
<th>쌍대 비교 시, 내용과 관계없이 첫 번째 위치에 제시된 답변을 선호하는
경향</th>
<th><sup>14</sup></th>
<th>- 답변 순서를 바꿔 두 번 평가하고 결과 평균화 <sup>49</sup></th>
<th>- 단일 답변 채점 방식 사용</th>
</tr>
<tr class="header">
<th><strong>장황함 편향 (Verbosity Bias)</strong></th>
<th>더 간결하고 핵심적인 답변보다 더 길고 상세한 답변을 선호하는
경향</th>
<th><sup>14</sup></th>
<th>- 평가 프롬프트에 '간결성'을 명시적인 평가 기준으로 포함 - 답변
길이를 평가 지표 중 하나로 별도 측정</th>
<th></th>
</tr>
<tr class="odd">
<th><strong>자기 선호 편향 (Self-Preference Bias)</strong></th>
<th>심판 모델이 자기 자신 또는 동일 계열 모델이 생성한 답변을 더 높게
평가하는 경향</th>
<th><sup>14</sup></th>
<th>- 답변 생성 모델과 평가 모델을 다른 것으로 사용 <sup>17</sup></th>
<th>- 여러 다른 심판 모델을 사용하여 결과 교차 검증</th>
</tr>
<tr class="header">
<th><strong>정확성 한계 (Correctness Limitation)</strong></th>
<th>심판 모델이 스스로 정답을 모르는 문제에 대해 답변의 사실적 정확성을
평가하지 못하는 문제</th>
<th><sup>13</sup></th>
<th>- 평가 프롬프트에 고품질의 인간 작성 참조 정답(reference answer)을
함께 제공 <sup>13</sup></th>
<th>- 사실 확인이 필요한 평가는 별도의 팩트체킹 도구와 연계</th>
</tr>
<tr class="odd">
<th><strong>비일관성 (Inconsistency)</strong></th>
<th>동일한 입력에 대해서도 평가를 실행할 때마다 다른 점수나 판단을
내리는 경향</th>
<th><sup>14</sup></th>
<th><p>- 모델의 temperature 파라미터를 0으로 설정하여 결정성 확보</p>
<p>- 여러 번 평가를 실행하여 평균 또는 다수결로 최종 점수 결정
15</p></th>
<th></th>
</tr>
</thead>
<tbody>
</tbody>
</table>

### **3.3 멀티턴 평가의 포괄적 과제** {#멀티턴-평가의-포괄적-과제}

개별 평가 기법의 한계를 넘어, 멀티턴 대화 평가라는 분야 자체가 직면한
근본적인 과제들이 있습니다.

**비용 대 확장성:** 고품질의 인간 평가는 비용이 많이 들고 규모를 키우기
어렵지만, 저비용의 자동화된 평가는 신뢰도에 한계가 있다는 근본적인 상충
관계가 존재합니다.^23^

**\'품질\'의 주관성:** \"좋은 대화\"를 구성하는 여러 요소, 예를 들어
참여도(engagement), 공감(empathy), 어조(tone) 등은 본질적으로 주관적이며
정량화하기 어렵습니다.^19^

**장기 기억 및 일관성:** 수십, 수백 턴에 걸친 매우 긴 대화에서 초반의
세부 사항을 기억하고 일관성을 유지하는 능력은 현재 LLM 기술의 최전선에
있는 과제입니다. 현재의 평가 벤치마크들은 이러한 장기 의존성을 엄격하게
테스트하기에는 대화 길이가 충분히 길지 않은 경우가 많습니다.^64^

**에이전트 행동 평가:** API 호출이나 데이터베이스 조회와 같은 외부
도구를 사용하는 챗봇의 평가는 더욱 복잡해집니다. 최종 응답의 실패가 언어
생성 능력의 문제가 아니라, 도구 사용 단계의 오류에서 비롯될 수 있기
때문입니다.^4^ 이는 TruLens와 같이 중간 실행 단계를 검사할 수 있는 평가
프레임워크의 필요성을 부각시킵니다.^45^

## **4부: 평가에서 진화로: 지속적 개선을 위한 프레임워크** {#부-평가에서-진화로-지속적-개선을-위한-프레임워크}

이 마지막 장에서는 평가 결과를 실제 제품 개선으로 연결하는 전략적
가이드를 제공합니다. 평가를 일회성 품질 검사가 아닌, 지속적인 제품
발전의 핵심 동력으로 만들기 위해 평가 프로세스를 운영 워크플로우에
내재화하는 방법을 다룹니다.

### **4.1 지속적 평가(CE) 파이프라인 구현 (LLMOps)** {#지속적-평가ce-파이프라인-구현-llmops}

**MLOps에서 LLMOps로의 전환:** 전통적인 머신러닝 운영(MLOps)의 원칙들이
LLM 애플리케이션에도 적용되지만, LLMOps는 고유한 과제를 제시합니다.
평가는 더 이상 정확도와 같은 단순한 수치로 측정되지 않으며, 예측
불가능한 텍스트 생성물을 주관적인 루브릭에 따라 평가해야 하는 복잡성을
가집니다.^22^

**오프라인 평가 (배포 전):**

- **골든 데이터셋 (Golden Datasets):** 제품의 대표적이고 도전적인 대화
  > 시나리오를 모아 \'골든 데이터셋\'을 구축하고 지속적으로 관리하는
  > 것은 필수적입니다.^66^ 이 데이터셋은 새로운 모델이나 프롬프트 변경
  > 시 성능 저하가 없는지 확인하는 기준선 역할을 합니다.

- **CI/CE/CD 워크플로우:** 자동화된 평가를 지속적 통합/지속적
  > 배포(CI/CD) 파이프라인에 통합합니다. 개발자가 새로운 프롬프트와 같은
  > 변경사항을 코드 저장소에 푸시하면, 자동화된 워크플로우가 트리거되어
  > 골든 데이터셋에 대한 평가(예: DeepEval, Ragas 점수 계산)를
  > 실행합니다. 이 평가 결과가 사전에 설정된 기준을 통과해야만
  > 변경사항이 배포될 수 있습니다.^66^

**온라인 평가 (배포 후):**

- **라이브 모니터링:** 운영 환경의 트래픽을 지속적으로 모니터링하여 성능
  > 저하, 개념 드리프트(concept drift), 또는 환각(hallucination) 및
  > 유해성 발현의 증가를 탐지합니다.^22^

- **피드백 루프:** 사용자가 직접 피드백(예: 좋아요/싫어요, 만족도
  > 평가)을 제공할 수 있는 메커니즘을 구현하고, 문제가 있는 대화는 인간
  > 검토자에게 전달되도록 시스템을 구축합니다.^18^ 이렇게 수집된 실제
  > 실패 사례들은 다음 오프라인 평가 주기를 위해 골든 데이터셋을
  > 보강하는 데 사용됩니다. DoorDash가 운영 환경에서 LLM 가드레일과 LLM
  > Judge를 사용하여 실시간으로 응답 품질을 모니터링하고 관리하는 사례는
  > 이러한 온라인 평가의 좋은 예시입니다.^71^

### **4.2 운영 환경 챗봇 QA를 위한 산업계 모범 사례** {#운영-환경-챗봇-qa를-위한-산업계-모범-사례}

이 섹션에서는 산업 현장의 가이드라인을 종합하여 실질적인 품질 보증(QA)
모범 사례를 제시합니다.

**검토 루틴 수립:** 하루에 1시간과 같이 정기적인 시간을 할애하여 운영
환경에서 발생한 대화 로그의 무작위 샘플을 수동으로 검토하는 루틴을
만듭니다. 이를 통해 자동화된 시스템이 놓칠 수 있는 오류 패턴이나 개선
영역을 발견할 수 있습니다.^72^

**분석 및 최적화:** 검토를 통해 발견된 인사이트를 활용하여 반복적으로
발생하는 문제(예: 특정 사용자 의도를 지속적으로 오해하는 경우)를
식별하고, 챗봇의 지식 베이스나 프롬프트를 최적화합니다.^72^

**투명성과 신뢰 구축:** 사용자에게 AI와 상호작용하고 있음을 명확히
알리고, 데이터 개인정보 보호 정책을 투명하게 공개하여 신뢰를 구축해야
합니다.^73^

**점진적 실패 및 이관 처리:** 챗봇이 인식하지 못하는 입력에 대해 적절히
대응하는 견고한 폴백(fallback) 전략을 마련하고, 필요 시 인간 상담원에게
원활하게 대화를 이관하는 프로세스를 갖추는 것이 중요합니다.^20^

**지속적인 학습 및 적응:** 챗봇은 \'한 번 설정하고 잊어버리는\' 시스템이
아닙니다. 실제 사용자 상호작용 데이터와 변화하는 비즈니스 요구사항에
따라 지속적인 모니터링, 테스트, 그리고 업데이트가 필요합니다.^72^

## **결론: 하이브리드 및 전체론적 평가 문화를 향하여**

본 보고서를 통해 멀티턴 대화형 AI를 평가하는 단 하나의 \'최고의\' 방법은
존재하지 않는다는 점이 명확해졌습니다. 현시점의 최첨단 평가 방식은
다양한 방법론의 강점을 전략적으로 결합한 \*\*하이브리드 접근법(hybrid
approach)\*\*입니다.

성공적인 평가는 자동화된 방법(대화 시뮬레이션, LLM-as-a-Judge)이
제공하는 규모의 경제와 인간의 감독(인간 주석, 사용자 피드백)이 제공하는
높은 신뢰도 사이의 균형을 맞추는 지속적이고 반복적인 프로세스입니다.

궁극적으로, 이 포괄적인 평가 프레임워크의 목표는 단순히 챗봇에 점수를
매기는 것을 넘어, 의미 있는 개선을 이끌어내는 긴밀한 피드백 루프를
만드는 것입니다. 이를 통해 평가는 단순한 품질 검사의 역할을 넘어, 더
신뢰할 수 있고, 효과적이며, 유용한 대화형 에이전트를 개발하는 핵심
엔진으로 자리매김하게 될 것입니다.

#### 참고 자료

1.  How to simulate multi-turn interactions - ️🛠️ LangSmith - LangChain,
    > 7월 28, 2025에 액세스,
    > [[https://docs.smith.langchain.com/evaluation/how_to_guides/multi_turn_simulation]{.underline}](https://docs.smith.langchain.com/evaluation/how_to_guides/multi_turn_simulation)

2.  LLMs Get Lost In Multi-Turn Conversation - arXiv, 7월 28, 2025에
    > 액세스,
    > [[https://arxiv.org/html/2505.06120v1]{.underline}](https://arxiv.org/html/2505.06120v1)

3.  Top LLM Chatbot Evaluation Metrics: Conversation Testing \..., 7월
    > 28, 2025에 액세스,
    > [[https://www.confident-ai.com/blog/llm-chatbot-evaluation-explained-top-chatbot-evaluation-metrics-and-testing-techniques]{.underline}](https://www.confident-ai.com/blog/llm-chatbot-evaluation-explained-top-chatbot-evaluation-metrics-and-testing-techniques)

4.  Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey -
    > arXiv, 7월 28, 2025에 액세스,
    > [[https://arxiv.org/html/2503.22458v1]{.underline}](https://arxiv.org/html/2503.22458v1)

5.  LLMs Get Lost In Multi-Turn Conversation - arXiv, 7월 28, 2025에
    > 액세스,
    > [[https://arxiv.org/pdf/2505.06120]{.underline}](https://arxiv.org/pdf/2505.06120)

6.  \[2505.06120\] LLMs Get Lost In Multi-Turn Conversation - arXiv, 7월
    > 28, 2025에 액세스,
    > [[https://arxiv.org/abs/2505.06120]{.underline}](https://arxiv.org/abs/2505.06120)

7.  LLMs Get Lost In Multi-Turn Conversation - Consensus, 7월 28, 2025에
    > 액세스,
    > [[https://consensus.app/papers/details/8ff7f82bbc5a58e5bd4555da58530b27/]{.underline}](https://consensus.app/papers/details/8ff7f82bbc5a58e5bd4555da58530b27/)

8.  LLMs Get Lost In Multi-Turn Conversation : r/LocalLLaMA - Reddit,
    > 7월 28, 2025에 액세스,
    > [[https://www.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/]{.underline}](https://www.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/)

9.  Best Practices for Evaluating Back-and-Forth Conversational AI -
    > PromptLayer, 7월 28, 2025에 액세스,
    > [[https://blog.promptlayer.com/best-practi-to-evaluate-back-and-forth-conversational-ai-in-promptlayer/]{.underline}](https://blog.promptlayer.com/best-practi-to-evaluate-back-and-forth-conversational-ai-in-promptlayer/)

10. Evaluating Multi-Step Conversational AI is Hard \| by Eoin Travers
    > \| Unmind Tech - Medium, 7월 28, 2025에 액세스,
    > [[https://medium.com/unmind-tech/evaluating-multi-step-conversational-ai-is-hard-029623f64263]{.underline}](https://medium.com/unmind-tech/evaluating-multi-step-conversational-ai-is-hard-029623f64263)

11. Simulating & Evaluating Multi turn Conversations - YouTube, 7월 28,
    > 2025에 액세스,
    > [[https://www.youtube.com/watch?v=2mRmiuyFRGc]{.underline}](https://www.youtube.com/watch?v=2mRmiuyFRGc)

12. A Survey on Multi-Turn Interaction Capabilities of Large Language
    > Models - arXiv, 7월 28, 2025에 액세스,
    > [[https://arxiv.org/html/2501.09959v1]{.underline}](https://arxiv.org/html/2501.09959v1)

13. No Free Labels: Limitations of LLM-as-a-Judge Without Human
    > Grounding - arXiv, 7월 28, 2025에 액세스,
    > [[https://arxiv.org/html/2503.05061v1]{.underline}](https://arxiv.org/html/2503.05061v1)

14. LLM-as-a-Judge Simply Explained: A Complete Guide to Run LLM Evals
    > at Scale, 7월 28, 2025에 액세스,
    > [[https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method]{.underline}](https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method)

15. LLM-as-a-judge: a complete guide to using LLMs for evaluations -
    > Evidently AI, 7월 28, 2025에 액세스,
    > [[https://www.evidentlyai.com/llm-guide/llm-as-a-judge]{.underline}](https://www.evidentlyai.com/llm-guide/llm-as-a-judge)

16. Evaluating Multi-turn Conversations - Ragas, 7월 28, 2025에 액세스,
    > [[https://docs.ragas.io/en/stable/howtos/applications/evaluating_multi_turn_conversations/]{.underline}](https://docs.ragas.io/en/stable/howtos/applications/evaluating_multi_turn_conversations/)

17. Are LLMs the best way to judge LLMs? - AIMon Labs, 7월 28, 2025에
    > 액세스,
    > [[https://www.aimon.ai/posts/llm-as-judge-pros-and-cons/]{.underline}](https://www.aimon.ai/posts/llm-as-judge-pros-and-cons/)

18. Optimize Virtual Assistant Performance with Multi-Turn Evaluation \|
    > Label Studio, 7월 28, 2025에 액세스,
    > [[https://labelstud.io/blog/evaluating-multi-turn-llm-chat-conversations-using-label-studio/]{.underline}](https://labelstud.io/blog/evaluating-multi-turn-llm-chat-conversations-using-label-studio/)

19. NLP Conversational Metrics Guide - Number Analytics, 7월 28, 2025에
    > 액세스,
    > [[https://www.numberanalytics.com/blog/nlp-conversational-metrics-guide]{.underline}](https://www.numberanalytics.com/blog/nlp-conversational-metrics-guide)

20. Rubric Development and Validation for Assessing Tasks\' Solving via
    > AI Chatbots - ERIC, 7월 28, 2025에 액세스,
    > [[https://files.eric.ed.gov/fulltext/EJ1434299.pdf]{.underline}](https://files.eric.ed.gov/fulltext/EJ1434299.pdf)

21. Understanding Human Evaluation Metrics in AI: What They Are and How
    > They Work, 7월 28, 2025에 액세스,
    > [[https://galileo.ai/blog/human-evaluation-metrics-ai]{.underline}](https://galileo.ai/blog/human-evaluation-metrics-ai)

22. LLMOps: Bringing LLMs into Production \| SuperAnnotate, 7월 28,
    > 2025에 액세스,
    > [[https://www.superannotate.com/blog/llm-operations-llmops]{.underline}](https://www.superannotate.com/blog/llm-operations-llmops)

23. Human Evaluation of Conversations is an Open Problem: comparing the
    > sensitivity of various methods for evaluating dialogue agent - ACL
    > Anthology, 7월 28, 2025에 액세스,
    > [[https://aclanthology.org/2022.nlp4convai-1.8.pdf]{.underline}](https://aclanthology.org/2022.nlp4convai-1.8.pdf)

24. RUBICON: Rubric-Based Evaluation of Domain-Specific Human-AI
    > Conversations - Microsoft, 7월 28, 2025에 액세스,
    > [[https://www.microsoft.com/en-us/research/wp-content/uploads/2024/05/RUBICON\_\_Rubric_Based_Evaluation_of_Domain_Specific_Human_AI_Conversations-8.pdf]{.underline}](https://www.microsoft.com/en-us/research/wp-content/uploads/2024/05/RUBICON__Rubric_Based_Evaluation_of_Domain_Specific_Human_AI_Conversations-8.pdf)

25. Better Automatic Evaluation of Open-Domain Dialogue Systems with
    > Contextualized Embeddings - ACL Anthology, 7월 28, 2025에 액세스,
    > [[https://aclanthology.org/W19-2310.pdf]{.underline}](https://aclanthology.org/W19-2310.pdf)

26. New Ways to Evaluate Open-Domain Conversational Agents - TOPBOTS,
    > 7월 28, 2025에 액세스,
    > [[https://www.topbots.com/conversational-ai-evaluation-metrics/]{.underline}](https://www.topbots.com/conversational-ai-evaluation-metrics/)

27. 15 Best A/B Testing Tools & Software in 2025 - VWO, 7월 28, 2025에
    > 액세스,
    > [[https://vwo.com/blog/ab-testing-tools/]{.underline}](https://vwo.com/blog/ab-testing-tools/)

28. What is the best A/B testing software? 2024 Recommendations &
    > Reviews, 7월 28, 2025에 액세스,
    > [[https://theproductmanager.com/tools/best-ab-testing-software/]{.underline}](https://theproductmanager.com/tools/best-ab-testing-software/)

29. Chatbot A/B Testing Guide: Boost Performance - Quidget, 7월 28,
    > 2025에 액세스,
    > [[https://quidget.ai/blog/ai-automation/chatbot-ab-testing-guide-boost-performance/]{.underline}](https://quidget.ai/blog/ai-automation/chatbot-ab-testing-guide-boost-performance/)

30. A/B Testing Examples For Product Growth - Userpilot, 7월 28, 2025에
    > 액세스,
    > [[https://userpilot.com/blog/ab-testing-examples/]{.underline}](https://userpilot.com/blog/ab-testing-examples/)

31. 16 A/B Testing Ideas to Boost Conversions + Tools to Use - HawkSEM,
    > 7월 28, 2025에 액세스,
    > [[https://hawksem.com/blog/ab-testing-ideas/]{.underline}](https://hawksem.com/blog/ab-testing-ideas/)

32. Multivariate Testing vs. A/B Testing: Comparison and Benefits - AB
    > Smartly, 7월 28, 2025에 액세스,
    > [[https://absmartly.com/blog/multivariate-testing-vs-ab-testing]{.underline}](https://absmartly.com/blog/multivariate-testing-vs-ab-testing)

33. A/B Testing vs. Multivariate Testing - Pros and Cons - Digital
    > Products Reviews, 7월 28, 2025에 액세스,
    > [[https://www.digitalproductsdp.com/blog/ab-testing-vs-multivariate-testing]{.underline}](https://www.digitalproductsdp.com/blog/ab-testing-vs-multivariate-testing)

34. A/B Testing Vs. Multivariate Testing: Which One Is Better - Invesp,
    > 7월 28, 2025에 액세스,
    > [[https://www.invespcro.com/ab-testing/vs-multivariate-testing/]{.underline}](https://www.invespcro.com/ab-testing/vs-multivariate-testing/)

35. Using an LLM as a Judge \| TrojAI, 7월 28, 2025에 액세스,
    > [[https://www.troj.ai/blog/using-llm-as-judge]{.underline}](https://www.troj.ai/blog/using-llm-as-judge)

36. Evaluation and Metrics in Conversational AI: A Comprehensive Guide,
    > 7월 28, 2025에 액세스,
    > [[https://www.xaqt.com/blog/conversational-ai-evaluation-and-metrics/]{.underline}](https://www.xaqt.com/blog/conversational-ai-evaluation-and-metrics/)

37. Contextual Awareness in Chatbots: Techniques & Best Practices - All
    > GPTs Directory, 7월 28, 2025에 액세스,
    > [[https://allgpts.co/blog/contextual-awareness-in-chatbots-techniques-and-best-practices/]{.underline}](https://allgpts.co/blog/contextual-awareness-in-chatbots-techniques-and-best-practices/)

38. MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language
    > Models in Multi-Turn Dialogues \| Request PDF - ResearchGate, 7월
    > 28, 2025에 액세스,
    > [[https://www.researchgate.net/publication/384215139_MT-Bench-101_A_Fine-Grained_Benchmark_for_Evaluating_Large_Language_Models_in_Multi-Turn_Dialogues]{.underline}](https://www.researchgate.net/publication/384215139_MT-Bench-101_A_Fine-Grained_Benchmark_for_Evaluating_Large_Language_Models_in_Multi-Turn_Dialogues)

39. Evaluating LLM-based chatbots: A comprehensive guide to performance
    > metrics \| by Shimin Zhang \| Data Science at Microsoft \| Medium,
    > 7월 28, 2025에 액세스,
    > [[https://medium.com/data-science-at-microsoft/evaluating-llm-based-chatbots-a-comprehensive-guide-to-performance-metrics-9c2388556d3e]{.underline}](https://medium.com/data-science-at-microsoft/evaluating-llm-based-chatbots-a-comprehensive-guide-to-performance-metrics-9c2388556d3e)

40. Measuring Chatbot Effectiveness: 16 KPIs to Track - Visiativ, 7월
    > 28, 2025에 액세스,
    > [[https://www.visiativ.com/en/actualites/news/measuring-chatbot-effectiveness/]{.underline}](https://www.visiativ.com/en/actualites/news/measuring-chatbot-effectiveness/)

41. Measuring ROI Of Conversational AI: Key Metrics & Strategies \|
    > mihup, 7월 28, 2025에 액세스,
    > [[https://mihup.ai/measuring-roi-of-conversational-ai-key-metrics-strategies/]{.underline}](https://mihup.ai/measuring-roi-of-conversational-ai-key-metrics-strategies/)

42. LLM Evaluation Frameworks: Head-to-Head Comparison - Comet, 7월 28,
    > 2025에 액세스,
    > [[https://www.comet.com/site/blog/llm-evaluation-frameworks/]{.underline}](https://www.comet.com/site/blog/llm-evaluation-frameworks/)

43. ‼️ Top 5 Open-Source LLM Evaluation Frameworks in 2025 - DEV
    > Community, 7월 28, 2025에 액세스,
    > [[https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m]{.underline}](https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m)

44. Evaluation how-to guides \| 🦜️🛠️ LangSmith - LangChain, 7월 28,
    > 2025에 액세스,
    > [[https://docs.smith.langchain.com/evaluation/how_to_guides]{.underline}](https://docs.smith.langchain.com/evaluation/how_to_guides)

45. TruLens: Evals and Tracing for Agents, 7월 28, 2025에 액세스,
    > [[https://www.trulens.org/]{.underline}](https://www.trulens.org/)

46. TruLens - Pinecone Docs, 7월 28, 2025에 액세스,
    > [[https://docs.pinecone.io/integrations/trulens]{.underline}](https://docs.pinecone.io/integrations/trulens)

47. Agent Evaluation: Using TruLens and AgentBench --- Part 2: Hands on
    > Implementation Details - Medium, 7월 28, 2025에 액세스,
    > [[https://medium.com/tr-labs-ml-engineering-blog/agent-evaluation-using-trulens-and-agentbench-part-2-hands-on-implementation-details-7a92515b4a7c]{.underline}](https://medium.com/tr-labs-ml-engineering-blog/agent-evaluation-using-trulens-and-agentbench-part-2-hands-on-implementation-details-7a92515b4a7c)

48. MT-Bench (Multi-turn Benchmark) - Klu.ai, 7월 28, 2025에 액세스,
    > [[https://klu.ai/glossary/mt-bench-eval]{.underline}](https://klu.ai/glossary/mt-bench-eval)

49. Benchmarking Amazon Nova: A comprehensive analysis through MT-Bench
    > and Arena-Hard-Auto \| Artificial Intelligence, 7월 28, 2025에
    > 액세스,
    > [[https://aws.amazon.com/blogs/machine-learning/benchmarking-amazon-nova-a-comprehensive-analysis-through-mt-bench-and-arena-hard-auto/]{.underline}](https://aws.amazon.com/blogs/machine-learning/benchmarking-amazon-nova-a-comprehensive-analysis-through-mt-bench-and-arena-hard-auto/)

50. MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language
    > Models in Multi-Turn Dialogues - ACL Anthology, 7월 28, 2025에
    > 액세스,
    > [[https://aclanthology.org/2024.acl-long.401/]{.underline}](https://aclanthology.org/2024.acl-long.401/)

51. \[2402.14762\] MT-Bench-101: A Fine-Grained Benchmark for Evaluating
    > Large Language Models in Multi-Turn Dialogues - arXiv, 7월 28,
    > 2025에 액세스,
    > [[https://arxiv.org/abs/2402.14762]{.underline}](https://arxiv.org/abs/2402.14762)

52. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena \|
    > OpenReview, 7월 28, 2025에 액세스,
    > [[https://openreview.net/forum?id=uccHPGDlao]{.underline}](https://openreview.net/forum?id=uccHPGDlao)

53. Chatbot Arena and the Elo rating system - Part 1 - Yi Zhu, 7월 28,
    > 2025에 액세스,
    > [[https://bryanyzhu.github.io/posts/2024-06-20-elo-part1/]{.underline}](https://bryanyzhu.github.io/posts/2024-06-20-elo-part1/)

54. How to Read Elo Ratings and Arena Scores for LLMs - Statology, 7월
    > 28, 2025에 액세스,
    > [[https://www.statology.org/how-to-read-elo-ratings-and-arena-scores-for-llms/]{.underline}](https://www.statology.org/how-to-read-elo-ratings-and-arena-scores-for-llms/)

55. Rating LLMs with ELO - the ChatBot Arena - Infos about AI, 7월 28,
    > 2025에 액세스,
    > [[https://ai-tasks.de/en/2024/01/rating-llms-with-elo-the-chatbot-arena/]{.underline}](https://ai-tasks.de/en/2024/01/rating-llms-with-elo-the-chatbot-arena/)

56. Elo vs Bradley-Terry: Which is Better for Comparing the Performance
    > of LLMs?, 7월 28, 2025에 액세스,
    > [[https://hippocampus-garden.com/elo_vs_bt/]{.underline}](https://hippocampus-garden.com/elo_vs_bt/)

57. Understanding the Ranking System in LLM Arena: How Models Are
    > Evaluated - Arsturn, 7월 28, 2025에 액세스,
    > [[https://www.arsturn.com/blog/understanding-the-ranking-system-in-llm-arena-how-models-are-evaluated]{.underline}](https://www.arsturn.com/blog/understanding-the-ranking-system-in-llm-arena-how-models-are-evaluated)

58. (PDF) How NOT To Evaluate Your Dialogue System: An Empirical Study
    > of Unsupervised Evaluation Metrics for Dialogue Response
    > Generation - ResearchGate, 7월 28, 2025에 액세스,
    > [[https://www.researchgate.net/publication/301837167_How_NOT_To_Evaluate_Your_Dialogue_System_An_Empirical_Study_of_Unsupervised_Evaluation_Metrics_for_Dialogue_Response_Generation]{.underline}](https://www.researchgate.net/publication/301837167_How_NOT_To_Evaluate_Your_Dialogue_System_An_Empirical_Study_of_Unsupervised_Evaluation_Metrics_for_Dialogue_Response_Generation)

59. SLIDE: A Framework Integrating Small and Large Language Models for
    > Open-Domain Dialogues Evaluation - arXiv, 7월 28, 2025에 액세스,
    > [[https://arxiv.org/html/2405.15924v3]{.underline}](https://arxiv.org/html/2405.15924v3)

60. LLM-based NLG Evaluation: Current Status and Challenges - arXiv, 7월
    > 28, 2025에 액세스,
    > [[https://arxiv.org/html/2402.01383v2]{.underline}](https://arxiv.org/html/2402.01383v2)

61. BERTScore -- A Powerful NLP Evaluation Metric Explained & How To
    > Tutorial In Python, 7월 28, 2025에 액세스,
    > [[https://spotintelligence.com/2024/08/20/bertscore/]{.underline}](https://spotintelligence.com/2024/08/20/bertscore/)

62. Improving Dialog Evaluation with a Multi-reference Adversarial
    > Dataset and Large Scale Pretraining \| Transactions of the
    > Association for Computational Linguistics - MIT Press Direct, 7월
    > 28, 2025에 액세스,
    > [[https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00347/96477/Improving-Dialog-Evaluation-with-a-Multi-reference]{.underline}](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00347/96477/Improving-Dialog-Evaluation-with-a-Multi-reference)

63. (PDF) Evaluating LLM-based Agents for Multi-Turn Conversations: A
    > Survey - ResearchGate, 7월 28, 2025에 액세스,
    > [[https://www.researchgate.net/publication/390321949_Evaluating_LLM-based_Agents_for_Multi-Turn_Conversations_A_Survey]{.underline}](https://www.researchgate.net/publication/390321949_Evaluating_LLM-based_Agents_for_Multi-Turn_Conversations_A_Survey)

64. MultiChallenge: A Realistic Multi-Turn Conversation Evaluation
    > Benchmark Challenging to Frontier LLMs - ACL Anthology, 7월 28,
    > 2025에 액세스,
    > [[https://aclanthology.org/2025.findings-acl.958.pdf]{.underline}](https://aclanthology.org/2025.findings-acl.958.pdf)

65. Recursively summarizing enables long-term dialogue memory in large
    > language models \| Request PDF - ResearchGate, 7월 28, 2025에
    > 액세스,
    > [[https://www.researchgate.net/publication/390703800_Recursively_summarizing_enables_long-term_dialogue_memory_in_large_language_models]{.underline}](https://www.researchgate.net/publication/390703800_Recursively_summarizing_enables_long-term_dialogue_memory_in_large_language_models)

66. Evaluating Large Language Model (LLM) systems: Metrics, challenges,
    > and best practices \| by Jane Huang \| Data Science at Microsoft
    > \| Medium, 7월 28, 2025에 액세스,
    > [[https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5]{.underline}](https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5)

67. What is MLOps? - IBM, 7월 28, 2025에 액세스,
    > [[https://www.ibm.com/think/topics/mlops]{.underline}](https://www.ibm.com/think/topics/mlops)

68. MLOps: Continuous delivery and automation pipelines in machine
    > learning - Google Cloud, 7월 28, 2025에 액세스,
    > [[https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning]{.underline}](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)

69. Integrating Static Quality Assurance in CI Chatbot Development
    > Workflows, 7월 28, 2025에 액세스,
    > [[https://www.computer.org/csdl/magazine/so/2024/05/10533225/1X1cIrN4ncI]{.underline}](https://www.computer.org/csdl/magazine/so/2024/05/10533225/1X1cIrN4ncI)

70. LLM Evaluation Step-by-Step Guide for Enterprise AI Success \|
    > Galileo, 7월 28, 2025에 액세스,
    > [[https://galileo.ai/blog/llm-evaluation-step-by-step-guide]{.underline}](https://galileo.ai/blog/llm-evaluation-step-by-step-guide)

71. 10 RAG examples and use cases from real companies - Evidently AI,
    > 7월 28, 2025에 액세스,
    > [[https://www.evidentlyai.com/blog/rag-examples]{.underline}](https://www.evidentlyai.com/blog/rag-examples)

72. Best practices guide for implementing a chatbot with generative AI,
    > 7월 28, 2025에 액세스,
    > [[https://www.aoc.cat/en/guia-de-bones-practiques-ia/]{.underline}](https://www.aoc.cat/en/guia-de-bones-practiques-ia/)

73. 18 chatbot best practices for your business - Freshworks, 7월 28,
    > 2025에 액세스,
    > [[https://www.freshworks.com/chatbots/best-practices/]{.underline}](https://www.freshworks.com/chatbots/best-practices/)

74. 8 Chatbot Best Practices and Tips - Talkdesk, 7월 28, 2025에 액세스,
    > [[https://www.talkdesk.com/blog/chatbot-best-practices/]{.underline}](https://www.talkdesk.com/blog/chatbot-best-practices/)

75. Chatbot Development Guide for Business Owners in 2025 - MobiDev, 7월
    > 28, 2025에 액세스,
    > [[https://mobidev.biz/blog/chatbot-development-guide]{.underline}](https://mobidev.biz/blog/chatbot-development-guide)
