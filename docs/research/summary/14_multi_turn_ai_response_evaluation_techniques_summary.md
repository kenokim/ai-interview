### **멀티턴 대화형 AI 평가 기법 요약**

이 문서는 단일 응답 평가를 넘어 여러 턴에 걸쳐 진행되는 동적 대화형 AI(챗봇 등)를 평가하기 위한 포괄적인 가이드입니다. 멀티턴 대화 평가는 개별 답변의 품질뿐만 아니라 대화 전체의 일관성, 맥락 유지, 기억력 등을 종합적으로 측정해야 하는 복잡한 과제이며, 특히 최신 LLM도 긴 대화에서 길을 잃는 현상 때문에 그 중요성이 부각되고 있습니다.

**주요 내용:**

1.  핵심 평가 방법론 (하이브리드 평가 퍼널):
    *   대화 시뮬레이션: 가상 사용자 LLM을 통해 자동화된 종단 간 테스트를 수행합니다. 개발 초기 단계의 신속한 반복 테스트에 유용합니다.
    *   LLM-as-a-Judge: 강력한 '심판' LLM을 사용해 대화 품질을 자동 채점합니다. 확장성이 뛰어나지만, 위치/장황함/자기 선호 등의 편향을 인지하고 완화 전략을 사용해야 합니다.
    *   인간 평가: 자동화 시스템이 놓치는 미묘한 뉘앙스를 평가하는 '최종 표준(Gold Standard)' 역할을 합니다. 자동화된 지표 검증에 필수적입니다.
    *   라이브 A/B 테스트: 실제 사용자 환경에서 여러 버전의 챗봇을 비교하여 비즈니스 KPI에 미치는 실질적인 영향을 측정합니다.
    *   최적 전략: 개발 단계에 따라 시뮬레이션 → LLM-as-a-Judge → 인간 평가 → A/B 테스트 순으로 결합하는 '하이브리드 평가 퍼널'을 구축하는 것이 가장 효과적입니다.

2.  다차원적 평가 지표 프레임워크:
    *   기초 품질 (턴 레벨): 정확성, 관련성, 유창성.
    *   맥락 및 기억력 (멀티턴 특화): `역할 고수`, `대화 관련성`, `지식 유지`, `맥락 유지`.
    *   과업 지향: `과업 완수율`, `목표 달성률(GCR)`, `최초 문의 해결률(FCR)`.
    *   사용자 경험 및 비즈니스: `사용자 만족도(CSAT)`, `순수 추천 지수(NPS)`, `이탈률`.
    *   핵심 과제: 내재적 품질 지표(예: 일관성)의 개선이 외재적 비즈니스 지표(예: CSAT)의 향상으로 어떻게 이어지는지 인과 관계를 규명하는 것이 최종 목표입니다.

3.  실무자 툴킷 (프레임워크 및 벤치마크):
    *   오픈소스 프레임워크:
        *   `DeepEval`: 유닛 테스트처럼 엄격한 평가.
        *   `Ragas`: RAG 파이프라인 평가에 특화.
        *   `LangSmith & OpenEvals`: LangChain 생태계와 긴밀히 통합된 관찰 가능성 플랫폼.
        *   `TruLens`: 에이전트의 내부 실행 단계(도구 호출, 계획)를 심층 분석.
    *   표준 벤치마크:
        *   `MT-Bench`: 다양한 영역에 걸쳐 멀티턴 대화 능력을 평가하는 표준화된 질문 세트.
        *   `Chatbot Arena`: 크라우드소싱을 통해 실제 사용자 선호도를 반영하는 Elo 평점 시스템.

4.  함정과 한계:
    *   전통적 지표의 한계: BLEU, ROUGE 등은 참조 답변과의 단어 일치만 보기 때문에 대화 품질 평가에 부적합합니다.
    *   지속적 개선(LLMOps): 평가는 일회성이 아니라, CI/CD 파이프라인에 통합되어 오프라인(골든 데이터셋) 및 온라인(라이브 모니터링, 사용자 피드백)에서 지속적으로 이루어져야 합니다.

**결론:**
효과적인 멀티턴 AI 평가는 단일 방법론이 아닌, 다양한 자동화 및 인간 평가 기법을 결합한 하이브리드 접근법을 요구합니다. 평가는 단순히 점수를 매기는 것을 넘어, 지속적인 제품 개선을 이끄는 핵심 피드백 루프로서 개발 워크플로우에 내재화되어야 합니다. 